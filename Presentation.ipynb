{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simple_id\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import gensim\n",
    "import sys\n",
    "%load_ext line_profiler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Exploration of computers, tech and R\"\n",
    "abstract = 'We used R and Python to do stuff. Then we did more stuff. Our code can be found on Github'\n",
    "w2vPath = 'w2v.zip'\n",
    "modelPath = 'data/BiRNN-2-128-260.pt'\n",
    "\n",
    "\n",
    "d, a = simple_id.makeVarArray(title, abstract, w2vPath, modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weightP': tensor(0.9183, device='cuda:0'),\n",
       " 'weightN': tensor(-0.9471, device='cuda:0'),\n",
       " 'probPos': tensor(0.9492),\n",
       " 'probNeg': tensor(0.1341),\n",
       " 'prediction': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous work: [http://lstm.seas.harvard.edu/client/index.html](http://lstm.seas.harvard.edu/client/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current interfaces:\n",
    "\n",
    "[Word2Vec visual](http://shiny.reidmcy.com/int/)\n",
    "\n",
    "[Records inspector](http://shiny.reidmcy.com/ntg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Exploration of computers, tech and R\"\n",
    "abstract = 'We used R and Python to do stuff. Then we did more stuff. Our code can be found on Github'\n",
    "w2vPath = 'w2v/w2v.bin'\n",
    "modelFname = 'data/BiRNN-2-128-260.pt'\n",
    "\n",
    "%lprun -f simple_id.makeVarArray a = simple_id.makeVarArray(title, abstract, w2vPath, modelFname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import tempfile\n",
    "import io\n",
    "\n",
    "\n",
    "with zipfile.ZipFile('w2v.zip', 'w') as myzip:\n",
    "    for i, word in enumerate(w2v.wv.vocab):\n",
    "        with tempfile.NamedTemporaryFile() as tmp:\n",
    "            np.save(tmp, w2v.wv[word])\n",
    "            tmp.seek(0)\n",
    "            try:\n",
    "                myzip.write(\n",
    "                    filename = tmp.name,\n",
    "                    arcname = word,\n",
    "                )\n",
    "            except:\n",
    "                print(word)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myzip =  zipfile.ZipFile('w2v.zip', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with myzip.open('and') as myfile:\n",
    "    f = io.BytesIO(myfile.read())\n",
    "    a = np.load(f)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tempfile.NamedTemporaryFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.seek(0)#(b\"SDFDFDFGFDD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.syn0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.word2vec.Word2Vec.load('w2v/w2v.bin')\n",
    "Net = torch.load(modelFname).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pandas.read_csv('data/Other social sciences.csv', error_bad_lines = False)\n",
    "df_sample.index = df_sample['wos_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking example record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_sample.loc['WOS:000206783400002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict = dict(row)\n",
    "row_dict['title_tokens'] = Tokens[row_dict['wos_id']][0]\n",
    "row_dict['abstract_tokens'] = Tokens[row_dict['wos_id']][1]\n",
    "\n",
    "\n",
    "row_dict['title_vecs'] = simple_id.genVecSeq(row_dict['title_tokens'], w2v)\n",
    "row_dict['abstract_vecs'] = simple_id.genVecSeq(row_dict['abstract_tokens'], w2v)\n",
    "row_dict['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDict = Net.predictRow(row_dict)\n",
    "predictionDict['wos_id'] = row['wos_id']\n",
    "predictionDict['title'] = row['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations Visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_id.compareRows([row_dict], Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict['title_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of varying input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(row_dict['title_tokens'])):\n",
    "    newDict = {\n",
    "    'abstract' : row_dict['abstract'],\n",
    "    'abstract_tokens' : row_dict['abstract_tokens'],\n",
    "    #'title' : row_dict['title'],\n",
    "    'title_tokens' : row_dict['title_tokens'][:i + 1],   \n",
    "    }\n",
    "    pred = Net.predictRow(newDict, w2v=w2v)\n",
    "    print(\"propP is: {:.2f}% with title: '{}'\".format(pred['probPos'] * 100, ' '.join(newDict['title_tokens'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(row_dict['abstract_tokens'])):\n",
    "    newDict = {\n",
    "    'abstract' : row_dict['abstract'],\n",
    "    'abstract_tokens' : row_dict['abstract_tokens'][:i + 1],\n",
    "    'title' : row_dict['title'],\n",
    "    'title_tokens' : row_dict['title_tokens'],   \n",
    "    }\n",
    "    pred = Net.predictRow(newDict, w2v=w2v)\n",
    "    print(\"propP is: {:.2f}% with abstract only {} sentences long\".format(pred['probPos'] * 100, i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1 = []\n",
    "for i in range(len(row_dict['title_tokens'])):\n",
    "    predT = []\n",
    "    for j in range(len(row_dict['abstract_tokens'])):\n",
    "        newDict = {\n",
    "            'abstract_tokens' : row_dict['abstract_tokens'][:j+1],\n",
    "            'title_tokens' : row_dict['title_tokens'][:i + 1],   \n",
    "            }\n",
    "        pred = Net.predictRow(newDict, w2v=w2v)\n",
    "        predT.append(float(pred['probPos']))\n",
    "    preds1.append(predT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "seaborn.heatmap(preds1, ax = ax, vmin=0, vmax=1, cmap='RdBu')\n",
    "ax.set_ylabel('Word title Terminated at')\n",
    "ax.set_xlabel('Number of Sentences in Abstract')\n",
    "ax.set_yticklabels(reversed(row_dict['title_tokens']),rotation = 35, fontsize = 20)\n",
    "row_dict['title_tokens']\n",
    "ax.set_title(\"Effects of varying input on DNN output probabilties\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(len(row_dict['title_tokens'])):\n",
    "    predT = []\n",
    "    for j in range(len(row_dict['abstract_tokens'])):\n",
    "        for k in range(len(row_dict['abstract_tokens'][j])):\n",
    "            newDict = {\n",
    "                'abstract_tokens' : row_dict['abstract_tokens'][:j] + [row_dict['abstract_tokens'][j][:k+1]],\n",
    "                'title_tokens' : row_dict['title_tokens'][:i + 1],   \n",
    "                }\n",
    "            pred = Net.predictRow(newDict, w2v=w2v)\n",
    "            predT.append(float(pred['probPos']))\n",
    "    print(\"{:.0f}% done\".format(i / len(row_dict['title_tokens']) * 100), end = '\\r')\n",
    "    preds.append(predT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "seaborn.heatmap(preds, ax = ax, vmin=0, vmax=1, cmap='RdBu')\n",
    "ax.set_ylabel('Word title Terminated at')\n",
    "ax.set_xlabel('Number of Words in Abstract')\n",
    "ax.set_yticklabels(reversed(row_dict['title_tokens']),rotation = 35, fontsize = 20)\n",
    "ax.set_title(\"Effects of varying Input on DNN output probabilties\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Mode Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Exploration of computers, tech and R\"\n",
    "abstract = 'We used R and Python to do stuff. Then we did more stuff. Our code can be found on Github'\n",
    "title2 = \"Effects of input elaboration on vocabulary acquisition through reading by Korean learners of English as a foreign language\"\n",
    "\n",
    "abstract2 = \"\"\"This article investigates whether (a) lexical elaboration (LE), typographical enhancement (TE), or a combination, and (b) explicit OF implicit,LE affect 297 Korean learner's acquisition of English vocabulary. The learners were asked to read one of six versions of an experimental text that contained 26 target words. The study adopted a 2 X 3 MANOVA (:design with TE and LE as two independent variables and form- and meaning-recognition vocabulary posttests as two dependent variables. The TE had two levels, enhanced and unenhanced, and the LE lipid three levels, explicit, implicit, and unelaborated. The results were (a) LE alone did not aid form recognition of vocabulary, (b) explicit LE alone aided meaning recognition of vocabulary, (c) TE alone did not aid form and meaning recognition of vocabulary, (d) LE and TE combined did not aid form recognition of vocabulary, (e) both explicit and implicit LE aided meaning recognition of vocabulary, (f) explicit and implicit LE (lid riot differ in their effect on form and meaning recognition of vocabulary, and (g) whether a text was further enhanced in addition to either explicit or implicit LE did not seem to affect the acquisition of the previously Unknown words' forms or meanings.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Exploration of humans, society and R\"\n",
    "abstract = ' Our code can be found on Github. We used methods and techniques to do stuff. Weber Freud and vocabulary acquisition were explored. Then we did more stuff.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleDict = {\n",
    "    'title_tokens' : simple_id.tokenizer(title),\n",
    "    'abstract_tokens' : simple_id.sentinizer(abstract),   \n",
    "    }\n",
    "pred = Net.predictRow(newDict, w2v=w2v)\n",
    "print(float(pred['probPos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predA = makeVaryingArray(exampleDict, Net, w2v)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "seaborn.heatmap(predA, ax = ax, vmin=0, vmax=1, cmap='RdBu', annot=True)\n",
    "ax.set_ylabel('Title went to ...')\n",
    "ax.set_xlabel('Abstract went to ...')\n",
    "ax.set_yticklabels(exampleDict['title_tokens'],rotation = 55, fontsize = 20)\n",
    "ax.set_xticklabels([' '.join(t)[:25] + '...' for t in exampleDict['abstract_tokens']],rotation = 0, fontsize = 10)\n",
    "ax.set_title(\"Effects of varying Input on output probabilty of computational\")\n",
    "plt.savefig('images/occ.pdf', format='pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleDict = {\n",
    "    'title_tokens' : simple_id.tokenizer(title2),\n",
    "    'abstract_tokens' : simple_id.sentinizer(abstract2),   \n",
    "    }\n",
    "pred = Net.predictRow(exampleDict, w2v=w2v)\n",
    "print(float(pred['probPos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predA = makeVaryingArray(exampleDict, Net, w2v)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "seaborn.heatmap(predA, ax = ax, vmin=0, vmax=1, cmap='RdBu')\n",
    "ax.set_xlabel('Number of Sentences in Abstract')\n",
    "ax.set_ylabel('Word title Terminated at')\n",
    "ax.set_yticklabels(reversed(exampleDict['title_tokens']),rotation = 35, fontsize = 20)\n",
    "ax.set_title(\"Effects of varying Input on DNN output probabilties\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleDict = {\n",
    "    'title_tokens' : simple_id.tokenizer(title2),\n",
    "    'abstract_tokens' : simple_id.sentinizer(abstract2 + ' Computer github code model python R C software simulation'),   \n",
    "    }\n",
    "pred = Net.predictRow(exampleDict, w2v=w2v)\n",
    "print(float(pred['probPos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predA = makeVaryingArray(exampleDict, Net, w2v)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "seaborn.heatmap(predA, ax = ax, vmin=0, vmax=1, cmap='RdBu')\n",
    "ax.set_xlabel('Number of Sentences in Abstract')\n",
    "ax.set_ylabel('Word title Terminated at')\n",
    "ax.set_yticklabels(reversed(exampleDict['title_tokens']),rotation = 35, fontsize = 20)\n",
    "ax.set_title(\"Effects of varying Input on DNN output probabilties\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the images for a bunch of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_sample.iloc[:5].iterrows():\n",
    "    print(row['title'])\n",
    "    row_dict = dict(row)\n",
    "    row_dict['title_tokens'] = Tokens[row_dict['wos_id']][0]\n",
    "    row_dict['abstract_tokens'] = Tokens[row_dict['wos_id']][1]\n",
    "    predA = makeVaryingArray(row_dict, Net, w2v)\n",
    "    fig, ax = plt.subplots(figsize = (15, 7))\n",
    "    seaborn.heatmap(predA, ax = ax, vmin=0, vmax=1, cmap='RdBu')\n",
    "    ax.set_xlabel('Number of Sentences in Abstract')\n",
    "    ax.set_ylabel('Word title Terminated at')\n",
    "    ax.set_yticklabels(reversed(row_dict['title_tokens']),rotation = 35, fontsize = 20)\n",
    "    ax.set_title(\"Effects of varying Input on DNN output probabilties:\\n{}\".format(row['title']))\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('images/pred_var_{}.pdf'.format(row['title'][:50].replace(' ', '_')), format = 'pdf')\n",
    "    \n",
    "for i, row in df_sample.iloc[-5:].iterrows():\n",
    "    print(row['title'])\n",
    "    predA = makeVaryingArray(row_dict, Net, w2v)\n",
    "    fig, ax = plt.subplots(figsize = (15, 7))\n",
    "    seaborn.heatmap(predA, ax = ax, vmin=0, vmax=1, cmap='RdBu')\n",
    "    ax.set_ylabel('Word title Terminated at')\n",
    "    ax.set_yticklabels(reversed(row_dict['title_tokens']),rotation = 35, fontsize = 20)\n",
    "    ax.set_xlabel('Number of Sentences in Abstract')\n",
    "    ax.set_title(\"Effects of varying Input on DNN output probabilties:\\n{}\".format(row['title']))\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('images/pred_var_{}.pdf'.format(row['title'][:50].replace(' ', '_').replace('/', '')), format = 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "+ Get interface with R setup\n",
    "+ Make interactive\n",
    "+ Add real explanations\n",
    "+ Maybe add more modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it truthful?\n",
    "\n",
    "Is it functional?\n",
    "\n",
    "Is it beautiful?\n",
    "\n",
    "Is it insightful?\n",
    "\n",
    "Is it enlightening?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
